# -*- coding: utf-8 -*-
"""medical insurence price prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1inKE8axD5_In1dQL1-FdLZyYGRzJbEeb
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as sc

df=pd.read_csv('insurance.csv')

df

"""This dataset contains 1338 data points with 6 independent features and 1 target feature(expenses).

"""

df.info()

"""From the above, we can see that the dataset contains 2 columns with float values 3 with categorical values and the rest contains integer values.

# Exploratory Data Analysis
"""

df.isnull().sum()

sns.heatmap(df.isnull())

"""So, here we can conclude that there are no null values in the dataset given.

###Bivariant analysis
"""

con=(df['age']>=40)
sns.barplot(x="age",y="expenses",data=df[con])
plt.title("Age VS Expenses")
plt.show()

features = ['sex', 'smoker', 'region']
plt.subplots(figsize=(10, 10))
for i, col in enumerate(features):
    plt.subplot(1, 3, i + 1)

    x = df[col].value_counts()
    plt.pie(x.values,
            labels=x.index,
            autopct='%1.1f%%')
plt.show()

pd.crosstab(df.children,df.smoker).plot(kind="bar",figsize=(30,10),fontsize=40)
plt.title('childern expenses')
plt.xlabel('age',fontsize=40)
plt.ylabel('Frequency',fontsize=40)
plt.legend(fontsize=40)
plt.show()

sns.kdeplot(df['age'],color='red',shade=True)

df.hist(figsize=(12,12))
plt.show()

con = df['expenses']>52000
sns.countplot(x='expenses', data= df[con])
plt.show()

"""###multivariant analysis

"""

sns.pairplot(df)

"""#Handling outliers"""

catcol = []
numcol = []
for i in df.dtypes.index:
  if df.dtypes[i] == "object":
    catcol.append(i)
  else:
    numcol.append(i)

catcol

numcol

"""Plot boxplot for numeric values for find out outliers from the column




"""

plt.figure(figsize=(10,10))
pltn=1
for i in numcol:
    if pltn <= 4:
        ax = plt.subplot(4,2,pltn)
        sns.boxplot(df[i])
        plt.xlabel(i)
        pltn=pltn+1
        plt.show()
plt.tight_layout()

"""Outliers can be handled by

1)IQR +- 1.5

2)IF Z-SCORE IS > 3 remove the outlier

IQR method causes more dataloss than zscore therefore We use z-score for removing outliers
"""

#for removing outlier use Z score
from scipy.stats import zscore
features=df[['age', 'bmi', 'children', 'expenses']]
z=np.abs(zscore(features))

z

newdf=df[(z<=3).all(axis=1)]

df.shape

newdf.shape

dataloss=(1338-1309)/1338
dataloss*100

"""# Handling Skewness"""

df.skew()

newdf.skew()

"""Plot distplot for numerical data to find out which data is normalized.



"""

plt.figure(figsize=(12,12))
pltn=1
for i in numcol:
  if pltn<=8:
    plt.subplot(4,2,pltn)
    sns.distplot(df[i])
    plt.xlabel(i)
  pltn=pltn+1
plt.tight_layout()

"""Now we check corr() of the features with the target to remove skewness of it."""

sns.heatmap(newdf.corr(),annot=True)

"""If skewness of columns with n correlation with target is >0.5 then remove skewness of those columns.

Form the above heatmap we conclude that none of the feature have good corr() with the target. so we have to remove skewness of all the columns.
"""

numcol

skew1 = ['age', 'bmi', 'children', 'expenses']
from sklearn.preprocessing import PowerTransformer
pt=PowerTransformer(method ="yeo-johnson")

newdf.head()

"""#Feature Encoding

For feature Encoding we use OrdinalEncoder()
"""

from sklearn.preprocessing import OrdinalEncoder
oe =OrdinalEncoder()
newdf[catcol] = oe.fit_transform(newdf[catcol])

newdf.head()

"""#Feature Scaling

It is used to normalize range of independent veriable.

Only done on features and not on labels.

We use StandardScalar for scaling.




"""

x=newdf.drop("expenses", axis =1)
y=newdf["expenses"]

x

y

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x = sc.fit_transform(x)
x

"""##Model Development"""

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=40)

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRFRegressor
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
from sklearn.model_selection import GridSearchCV

def mymodel(model):
    model.fit(xtrain,ytrain)
    ypred=model.predict(xtest)
    train=model.score(xtrain,ytrain)
    test=model.score(xtest,ytest)

    print(f"Traning accuracy:{train}\nTesting accuracy:{test}\n\n")
    mae=mean_absolute_error(ytest,ypred)
    print("MAE :",mae)
    mse=mean_squared_error(ytest,ypred)
    print("MSE :",mse)
    rmse=np.sqrt(mse)
    print("RMSE :",rmse)
    r2=r2_score(ytest,ypred)
    print("R2_Score",r2)
    avg_rmse=np.mean(rmse)
    print("Average RMSE:", avg_rmse)
    return model

lr = mymodel(LinearRegression())
print("CVS:",cross_val_score(lr,xtrain,ytrain,scoring="r2",cv=5).mean())

dc = mymodel(DecisionTreeRegressor())

svr = mymodel(SVR())

knn = mymodel(KNeighborsRegressor(algorithm= 'ball_tree', n_neighbors= 10, p= 2, weights= 'uniform'))
"""param_grid={ 'n_neighbors': [3, 5, 10, 20,30,50],
            'weights': ['uniform', 'distance'],
             'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
             'p': [1, 2]
}
grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(xtrain, ytrain)
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')"""

#### Final model
xg = mymodel(XGBRFRegressor(n_estimators=500,max_depth=4,learning_rate=1,colsample_bynode=0.8))
print(cross_val_score(xg,xtrain,ytrain,cv=5).mean())

pf=PolynomialFeatures(degree=2)
xp=pf.fit_transform(x)
lr=LinearRegression()
lr.fit(xp,y)
ypred=lr.predict(xp)

mae=mean_absolute_error(y,ypred)
mse=mean_squared_error(y,ypred)
rmse=np.sqrt(mse)
r2=r2_score(y,ypred)
print(f"MAE:{mae}\nMSE:{mse}\nRMSE:{rmse}\nAccuracy:{r2}")

GBR = mymodel(GradientBoostingRegressor())
"""param_grid = {
    'learning_rate': [0.01, 0.1, 0.2,0.5],
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(GBR, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(xtrain, ytrain)
best_params = grid_search.best_params_
print(f'Best Hyperparameters: {best_params}')"""

GBR = mymodel(GradientBoostingRegressor(learning_rate= 0.2, max_depth= 10, min_samples_leaf= 20, min_samples_split= 20, n_estimators= 100))

GBR = mymodel(GradientBoostingRegressor(subsample=1,max_features=15))
scores = cross_val_score(GBR, x, y, cv=5, scoring='neg_mean_squared_error')

import numpy as np
train_scores = []
test_scores = []

for i in range(1, 201):
    l1 = Lasso(alpha=i)
    l1.fit(xtrain, ytrain)
    train_score = l1.score(xtrain, ytrain)
    test_score = l1.score(xtest, ytest)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"{i}  {train_score}  {test_score}")

rf = mymodel(RandomForestRegressor(max_depth= 8, max_features= "auto", min_samples_leaf= 6, min_samples_split= 8, n_estimators= 40))

import numpy as np
train_scores = []
test_scores = []

for i in range(1, 201):
    l1 = Lasso(alpha=i)
    l1.fit(xtrain, ytrain)
    train_score = l1.score(xtrain, ytrain)
    test_score = l1.score(xtest, ytest)
    train_scores.append(train_score)
    test_scores.append(test_score)
    print(f"{i}  {train_score}  {test_score}")

"""After trying all the algorithms we got good accuracy in XGBRFRegressor() that is 0.8416954298536888. so we will use XGBRFRegressor() algo in the web application."""

df

newdf

df['region'].value_counts()

newdf['region'].value_counts()

